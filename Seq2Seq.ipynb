{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "tian3XrG80Dg",
        "edDjmjvH8YYo",
        "joclhRIWSZUy",
        "5mhzOTbNS1DX",
        "vOXAlul8bNOL",
        "mam-BLyIS_u6",
        "pVXAHnI4UF9I"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io2MoQ_KsrC6",
        "outputId": "40b1b9ca-04b3-4ab1-9357-467ad13c5a75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# please navigate to the folder where you will place the folder \"data\"\n",
        "%cd <path-to-data-folder>\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/DLE_assignment/DLE_assignment\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY3sipYj2q2-"
      },
      "source": [
        "## English to French translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv6KTmUq2q3D"
      },
      "source": [
        "We start with the Vanilla Seq2Seq model and try out a few architectural changes to achieve better performance.\n",
        "\n",
        "The Seq2Seq model is introduced in this publication: https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgIlOG7LZD-H"
      },
      "source": [
        "### SOLUTION :\n",
        "\n",
        "**Starting point**\n",
        "\n",
        "1. Vanilla Seq2Seq model with a single layer and  implemented using LSTM cells. \n",
        "\n",
        "**Changes made and things tried out :**\n",
        "\n",
        "1. Two Seq2Seq models using **LSTM and GRU models** have been trained. \n",
        "2. Bidirectional RNNs have been used and has proven to be more effective. **Only the encoders have been made bidirectional.**\n",
        "3. Deeper LSTM/GRU models have been used to create the Seq2Seq network. Specifically, **3 layer networks** have been created for both LSTM and GRU based frameworks.\n",
        "4. Initialize LSTM/GRU units with a **uniform distribution** between -0.08 and 0.08 (according to Seq2Seq paper). \n",
        "5. Using ReLU activation function instead of tanh **did not help.**\n",
        "6.**Input string was reversed** when feeding to the encoder (according to Seq2Seq paper). \n",
        "7. **Using BLEU score to quantify performance.** Taking the average of the BLEU scores of random 1000 sentences in the dataset.\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "1. Usage of dropout results in a lower validation error, but doesnt exactly yield a significantly better BLEU score. \n",
        "2. LSTM (BLEU: ~0.36) gives a better performance compared to GRU (BLEU: ~0.32). \n",
        "3. The saved weights are for the scenarios of a) normal Bi directional RNN, b) Bi directional string reversed RNN with dropout and c) Bi directional string reversed RNN without dropout\n",
        "\n",
        "**Note:**\n",
        "\n",
        "Attention mechanism has **not** been used because of the huge increase in computational complexity and little increase in performance, especially since this is a Character-level RNN. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tian3XrG80Dg"
      },
      "source": [
        "**################################################################**\n",
        "\n",
        "### DATA PREPROCESS\n",
        "\n",
        "**################################################################**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-74S22f_2q3I"
      },
      "source": [
        "import random\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTF07-hP2q3X",
        "outputId": "95a83583-17d3-40fc-a008-e6af557473cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# TRAINING DETAILS\n",
        "\n",
        "batch_size = 64       # Batch size for training.\n",
        "epochs = 30           # Number of epochs to train for.\n",
        "latent_dim = 256      # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000   # Number of samples to train on.\n",
        "data_path = './data/fra.txt' # Path to the data txt file on disk.\n",
        "\n",
        "# OPEN FILE AND READ SENTENCES\n",
        "\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text = line.split('\\t')\n",
        "    input_text = input_text[::-1] # reverse input sequence\n",
        "    \n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "# PRINT DATA DETAILS\n",
        "\n",
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
        "\n",
        "# SET UP DATA\n",
        "\n",
        "input_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
        "            \n",
        "# REVERSE LOOKUP TOKENS\n",
        "\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 10000\n",
            "Number of unique input tokens: 71\n",
            "Number of unique output tokens: 94\n",
            "Max sequence length for inputs: 16\n",
            "Max sequence length for outputs: 59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edDjmjvH8YYo"
      },
      "source": [
        "**################################################################**\n",
        "\n",
        "### LSTM BASED MODEL\n",
        "\n",
        "**################################################################**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPtSZti42q4T"
      },
      "source": [
        "import keras # 2.2.4\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, LSTM, GRU, Dense\n",
        "from keras.layers import Bidirectional, Concatenate\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import Lambda\n",
        "from keras import backend as K\n",
        "from keras.initializers import RandomUniform\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOqqkKt12q4R"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kw54yMVT2q4Z",
        "outputId": "3faed6a9-05b3-480a-ca4d-28c5011224b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# # Define an input sequence and process it.\n",
        "# encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "# encoder = LSTM(latent_dim, return_state=True)\n",
        "# encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "# # We discard `encoder_outputs` and only keep the states.\n",
        "# encoder_states = [state_h, state_c]\n",
        "\n",
        "# # Set up the decoder, using `encoder_states` as initial state.\n",
        "# decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "# # We set up our decoder to return full output sequences,\n",
        "# # and to return internal states as well. We don't use the\n",
        "# # return states in the training model, but we will use them in inference.\n",
        "# decoder = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "# decoder_outputs, _, _ = decoder(decoder_inputs, initial_state=encoder_states)\n",
        "# decoder_dense = Dense(num_decoder_tokens, activation='softmax')# ,kernel_regularizer=rglz.l2(0.01))\n",
        "# decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# # Define the model that will turn\n",
        "# # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "# model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "#######################################\n",
        "### Stacked Bidirectional LSTM model\n",
        "#######################################\n",
        "\n",
        "# Define all layers.\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "\n",
        "encoder_1 = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True, kernel_initializer=RandomUniform(minval=-0.08, maxval=0.08, seed=None)), merge_mode='concat')\n",
        "encoder_2 = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True, kernel_initializer=RandomUniform(minval=-0.08, maxval=0.08, seed=None)), merge_mode='concat')\n",
        "encoder_3 = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True, kernel_initializer=RandomUniform(minval=-0.08, maxval=0.08, seed=None)), merge_mode='concat')\n",
        "\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "decoder_1 = LSTM(latent_dim*2, return_sequences=True, return_state=True, kernel_initializer=RandomUniform(minval=-0.08, maxval=0.08, seed=None))\n",
        "decoder_2 = LSTM(latent_dim*2, return_sequences=True, return_state=True, kernel_initializer=RandomUniform(minval=-0.08, maxval=0.08, seed=None))\n",
        "decoder_3 = LSTM(latent_dim*2, return_sequences=True, return_state=True, kernel_initializer=RandomUniform(minval=-0.08, maxval=0.08, seed=None))\n",
        "\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "\n",
        "# Encoder\n",
        "encoder_outputs, ehf1, ecf1, ehb1, ecb1 = encoder_1(encoder_inputs)\n",
        "encoder_outputs, ehf2, ecf2, ehb2, ecb2 = encoder_2(encoder_outputs)\n",
        "encoder_outputs, ehf3, ecf3, ehb3, ecb3 = encoder_3(encoder_outputs)\n",
        "\n",
        "eh1 = Concatenate()([ehf1, ehb1])\n",
        "ec1 = Concatenate()([ecf1, ecb1])\n",
        "eh2 = Concatenate()([ehf2, ehb2])\n",
        "ec2 = Concatenate()([ecf2, ecb2])\n",
        "eh3 = Concatenate()([ehf3, ehb3])\n",
        "ec3 = Concatenate()([ecf3, ecb3])\n",
        "\n",
        "encoder_states = [eh1, ec1, eh2, ec2, eh3, ec3]\n",
        "\n",
        "# Decoder\n",
        "\n",
        "decoder_outputs,_,_ = decoder_1(decoder_inputs, initial_state=[eh1, ec1])\n",
        "decoder_outputs,_,_ = decoder_2(decoder_outputs, initial_state=[eh2, ec2]) \n",
        "decoder_outputs,_,_ = decoder_3(decoder_outputs, initial_state=[eh3, ec3])\n",
        "\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV3aTxWpa9oM"
      },
      "source": [
        "#### Training Phase - Run only when training model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XopUon6C2q4x"
      },
      "source": [
        "# from keras import optimizers\n",
        "# sgd = optimizers.SGD(lr=0.7, clipnorm=5.)\n",
        "# model.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=30,\n",
        "          callbacks=[EarlyStopping(patience=3, verbose=1), ReduceLROnPlateau(patience=1, min_lr=10e-6, verbose=1)],\n",
        "          validation_split=0.2)\n",
        "\n",
        "model.save_weights('my_trained_model_BiLSTM_rev_noDropout.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2B6M1q92q4s"
      },
      "source": [
        "####  Inference phase\n",
        "\n",
        "1. Encode input and retrieve initial decoder state\n",
        "2. Run one step of decoder with this initial state and a \"start of sequence\" token as target. Output will be the next target token\n",
        "3. Repeat with the current target token and current states"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbm715Rk2q5B"
      },
      "source": [
        "# model = load_model('my_trained_model_LSTM.h5')\n",
        "# model = load_model('my_trained_model_BiLSTM.h5')\n",
        "model.load_weights('BiLSTM_rev.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4hbxiy92q5Q"
      },
      "source": [
        "# def decode_sequence(input_seq, encoder_model, decoder_model):\n",
        "#     # Encode the input as state vectors.\n",
        "#     states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "#     # Generate empty target sequence of length 1.\n",
        "#     target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "#     # Populate the first character of target sequence with the start character.\n",
        "#     target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "#     # Sampling loop for a batch of sequences\n",
        "#     # (to simplify, here we assume a batch of size 1).\n",
        "#     stop_condition = False\n",
        "#     decoded_sentence = ''\n",
        "#     while not stop_condition:\n",
        "#         output_tokens, h, c = decoder_model.predict(\n",
        "#             [target_seq] + states_value)\n",
        "\n",
        "#         # Sample a token\n",
        "#         sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "#         sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "#         decoded_sentence += sampled_char\n",
        "\n",
        "#         # Exit condition: either hit max length\n",
        "#         # or find stop character.\n",
        "#         if (sampled_char == '\\n' or\n",
        "#            len(decoded_sentence) > max_decoder_seq_length):\n",
        "#             stop_condition = True\n",
        "\n",
        "#         # Update the target sequence (of length 1).\n",
        "#         target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "#         target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "#         # Update states\n",
        "#         states_value = [h, c]\n",
        "\n",
        "#     return decoded_sentence\n",
        "\n",
        "\n",
        "##############################################\n",
        "### STACKED DECODE SEQUENCE BIDIRECTIONAL\n",
        "##############################################\n",
        "\n",
        "def decode_sequence(input_seq, encoder_model, decoder_model):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h1, c1, h2, c2, h3, c3 = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h1, c1, h2, c2, h3, c3]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OOER6eJ2q5f"
      },
      "source": [
        "# Define sampling models\n",
        "\n",
        "# encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# print(decoder_inputs.shape)\n",
        "\n",
        "# decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "# decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "# decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "# print(decoder_states_inputs[0].shape, decoder_states_inputs[1].shape)\n",
        "\n",
        "# decoder_outputs, state_h, state_c = decoder(\n",
        "#     decoder_inputs, initial_state=decoder_states_inputs)\n",
        "# print(decoder_outputs.shape)\n",
        "\n",
        "# decoder_states = [state_h, state_c]\n",
        "# print(decoder_states[0].shape, decoder_states[1].shape)\n",
        "\n",
        "# decoder_outputs = decoder_dense(decoder_outputs)\n",
        "# print(decoder_outputs.shape)\n",
        "\n",
        "# decoder_model = Model(\n",
        "#     [decoder_inputs] + decoder_states_inputs,\n",
        "#     [decoder_outputs] + decoder_states)\n",
        "\n",
        "\n",
        "# #######################################################\n",
        "# ### STACKED LSTM INFERENCE\n",
        "# #######################################################\n",
        "\n",
        "# encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# decoder_state_input_h1 = Input(shape=(latent_dim,))\n",
        "# decoder_state_input_c1 = Input(shape=(latent_dim,))\n",
        "# decoder_state_input_h2 = Input(shape=(latent_dim,))\n",
        "# decoder_state_input_c2 = Input(shape=(latent_dim,))\n",
        "# decoder_state_input_h3 = Input(shape=(latent_dim,))\n",
        "# decoder_state_input_c3 = Input(shape=(latent_dim,))\n",
        "\n",
        "# decoder_states_inputs = [decoder_state_input_h1, decoder_state_input_c1,\n",
        "#                         decoder_state_input_h2, decoder_state_input_c2,\n",
        "#                         decoder_state_input_h3, decoder_state_input_c3]\n",
        "\n",
        "\n",
        "# decoder_outputs, dh1, dc1 = decoder_1(decoder_inputs, initial_state=decoder_states_inputs[0:2])\n",
        "# decoder_outputs, dh2, dc2 = decoder_2(decoder_outputs, initial_state=decoder_states_inputs[2:4])\n",
        "# decoder_outputs, dh3, dc3 = decoder_3(decoder_outputs, initial_state=decoder_states_inputs[4:])                            \n",
        "\n",
        "# decoder_states = [dh1, dc1, dh2, dc2, dh3, dc3]\n",
        "\n",
        "\n",
        "# decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# decoder_model = Model(\n",
        "#     [decoder_inputs] + decoder_states_inputs,\n",
        "#     [decoder_outputs] + decoder_states)\n",
        "\n",
        "\n",
        "#######################################################\n",
        "### STACKED LSTM INFERENCE BIDIRECTIONAL\n",
        "#######################################################\n",
        "\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h1 = Input(shape=(latent_dim*2,))\n",
        "decoder_state_input_c1 = Input(shape=(latent_dim*2,))\n",
        "decoder_state_input_h2 = Input(shape=(latent_dim*2,))\n",
        "decoder_state_input_c2 = Input(shape=(latent_dim*2,))\n",
        "decoder_state_input_h3 = Input(shape=(latent_dim*2,))\n",
        "decoder_state_input_c3 = Input(shape=(latent_dim*2,))\n",
        "\n",
        "decoder_states_inputs = [decoder_state_input_h1, decoder_state_input_c1,\n",
        "                        decoder_state_input_h2, decoder_state_input_c2,\n",
        "                        decoder_state_input_h3, decoder_state_input_c3]\n",
        "\n",
        "\n",
        "decoder_outputs, dh1, dc1 = decoder_1(decoder_inputs, initial_state=decoder_states_inputs[0:2])\n",
        "decoder_outputs, dh2, dc2 = decoder_2(decoder_outputs, initial_state=decoder_states_inputs[2:4])\n",
        "decoder_outputs, dh3, dc3 = decoder_3(decoder_outputs, initial_state=decoder_states_inputs[4:])                            \n",
        "\n",
        "decoder_states = [dh1, dc1, dh2, dc2, dh3, dc3]\n",
        "\n",
        "\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTvGUNok2q5y"
      },
      "source": [
        "####  Test sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFoXKtI82q50"
      },
      "source": [
        "sample_idx = random.sample(range(len(input_texts)), 1000)\n",
        "test_texts = ['.sknahT', '.olleH', '?uoy era woH'] + [input_texts[i] for i in sample_idx]\n",
        "test_outputs = ['Merci.', 'Bonjour.', 'Comment vas-tu?'] + [target_texts[i] for i in sample_idx]\n",
        "\n",
        "def encode_texts_to_1hot_seq(input_texts):\n",
        "    input_seq = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
        "    for i, text in enumerate(input_texts):\n",
        "        for t, char in enumerate(text):\n",
        "            input_seq[i, t, input_token_index[char]] = 1.\n",
        "    return input_seq\n",
        "\n",
        "i=0\n",
        "BLEUscore=0\n",
        "\n",
        "for test_text, test_output in zip(test_texts, test_outputs):\n",
        "  \n",
        "    # Take one sequence (part of the training set) for trying out decoding.\n",
        "    input_seq = encode_texts_to_1hot_seq([test_text])\n",
        "    decoded_sentence = decode_sequence(input_seq, encoder_model, decoder_model)\n",
        "    \n",
        "    # BLEU score\n",
        "    BLEUscore += nltk.translate.bleu_score.sentence_bleu([decoded_sentence.split(\" \")], test_output.split(\" \"), weights = [0.5, 0.5, 0, 0]) #(0.5, 0.5)\n",
        "    i+=1\n",
        "    \n",
        "#     print('---------------------------------------------------------')\n",
        "#     print('Input sentence: ', test_text[::-1])\n",
        "#     print('Decoded sentence: ', decoded_sentence)\n",
        "#     print('Actual translation: ', test_output)\n",
        "\n",
        "\n",
        "print(\"The BLEU score for this dataset is: \", BLEUscore*1.0/i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joclhRIWSZUy"
      },
      "source": [
        "**################################################################**\n",
        "\n",
        "### GRU BASED MODEL\n",
        "\n",
        "**################################################################**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjAbLT3phhal"
      },
      "source": [
        "import keras # 2.2.4\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, LSTM, GRU, Dense\n",
        "from keras.layers import Bidirectional, Concatenate\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import Lambda\n",
        "from keras import backend as K\n",
        "from keras.initializers import RandomUniform\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mhzOTbNS1DX"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvBDMrCmSiEn"
      },
      "source": [
        "#######################################\n",
        "### Stacked Bidirectional GRU model\n",
        "#######################################\n",
        "\n",
        "# Define all layers.\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "\n",
        "encoder_1 = Bidirectional(GRU(latent_dim, return_state=True, return_sequences=True, kernel_initializer=RandomUniform(minval=-0.08, maxval=0.08, seed=None)), merge_mode='concat')\n",
        "encoder_2 = Bidirectional(GRU(latent_dim, return_state=True, return_sequences=True, kernel_initializer=RandomUniform(minval=-0.08, maxval=0.08, seed=None)), merge_mode='concat')\n",
        "encoder_3 = Bidirectional(GRU(latent_dim, return_state=True, return_sequences=True, kernel_initializer=RandomUniform(minval=-0.08, maxval=0.08, seed=None)), merge_mode='concat')\n",
        "\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "decoder_1 = GRU(latent_dim*2, return_sequences=True, return_state=True, kernel_initializer=RandomUniform(minval=-0.08, maxval=0.08, seed=None))\n",
        "decoder_2 = GRU(latent_dim*2, return_sequences=True, return_state=True, kernel_initializer=RandomUniform(minval=-0.08, maxval=0.08, seed=None))\n",
        "decoder_3 = GRU(latent_dim*2, return_sequences=True, return_state=True, kernel_initializer=RandomUniform(minval=-0.08, maxval=0.08, seed=None))\n",
        "\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "\n",
        "# Encoder\n",
        "encoder_outputs, ehf1, ehb1 = encoder_1(encoder_inputs) \n",
        "\n",
        "encoder_outputs, ehf2, ehb2 = encoder_2(encoder_outputs)\n",
        "\n",
        "encoder_outputs, ehf3, ehb3 = encoder_3(encoder_outputs)\n",
        "\n",
        "eh1 = Concatenate()([ehf1, ehb1])\n",
        "eh2 = Concatenate()([ehf2, ehb2])\n",
        "eh3 = Concatenate()([ehf3, ehb3])\n",
        "\n",
        "encoder_states = [eh1, eh2, eh3]\n",
        "\n",
        "\n",
        "# Decoder\n",
        "\n",
        "decoder_outputs,_ = decoder_1(decoder_inputs, initial_state=eh1)\n",
        "decoder_outputs,_ = decoder_2(decoder_outputs, initial_state=eh2)\n",
        "decoder_outputs,_ = decoder_3(decoder_outputs, initial_state=eh3)\n",
        "\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOXAlul8bNOL"
      },
      "source": [
        "#### Training Phase - Run only when training model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6wU3LFQUkZr",
        "outputId": "185e900c-cd7f-45d2-c41b-d56066c2ee7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        }
      },
      "source": [
        "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          callbacks=[EarlyStopping(patience=3, verbose=1), ReduceLROnPlateau(patience=1, min_lr=10e-6, verbose=1)],\n",
        "          validation_split=0.2)\n",
        "\n",
        "model.save_weights('BiGRU_rev.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/30\n",
            "8000/8000 [==============================] - 40s 5ms/step - loss: 0.7447 - val_loss: 0.6705\n",
            "Epoch 2/30\n",
            "8000/8000 [==============================] - 32s 4ms/step - loss: 0.4826 - val_loss: 0.5377\n",
            "Epoch 3/30\n",
            "8000/8000 [==============================] - 33s 4ms/step - loss: 0.3909 - val_loss: 0.4800\n",
            "Epoch 4/30\n",
            "8000/8000 [==============================] - 32s 4ms/step - loss: 0.3266 - val_loss: 0.4322\n",
            "Epoch 5/30\n",
            "8000/8000 [==============================] - 32s 4ms/step - loss: 0.2786 - val_loss: 0.4064\n",
            "Epoch 6/30\n",
            "8000/8000 [==============================] - 33s 4ms/step - loss: 0.2389 - val_loss: 0.3920\n",
            "Epoch 7/30\n",
            "8000/8000 [==============================] - 32s 4ms/step - loss: 0.2055 - val_loss: 0.3862\n",
            "Epoch 8/30\n",
            "8000/8000 [==============================] - 33s 4ms/step - loss: 0.1771 - val_loss: 0.3888\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 9/30\n",
            "8000/8000 [==============================] - 32s 4ms/step - loss: 0.1277 - val_loss: 0.3709\n",
            "Epoch 10/30\n",
            "8000/8000 [==============================] - 33s 4ms/step - loss: 0.1164 - val_loss: 0.3720\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Epoch 11/30\n",
            "8000/8000 [==============================] - 33s 4ms/step - loss: 0.1082 - val_loss: 0.3720\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Epoch 12/30\n",
            "8000/8000 [==============================] - 32s 4ms/step - loss: 0.1073 - val_loss: 0.3721\n",
            "Epoch 00012: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mam-BLyIS_u6"
      },
      "source": [
        "#### Inference phase\n",
        "\n",
        "1. Encode input and retrieve initial decoder state\n",
        "2. Run one step of decoder with this initial state and a \"start of sequence\" token as target. Output will be the next target token\n",
        "3. Repeat with the current target token and current states"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4qb9WQXUnem"
      },
      "source": [
        "model.load_weights('BiGRU_rev.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uuXwmmzS32x"
      },
      "source": [
        "def decode_sequence_gru(input_seq, encoder_model, decoder_model):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    \n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h1, h2, h3 = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h1, h2, h3]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUxaZ6RXTHlt"
      },
      "source": [
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h1 = Input(shape=(latent_dim*2,))\n",
        "decoder_state_input_h2 = Input(shape=(latent_dim*2,))\n",
        "decoder_state_input_h3 = Input(shape=(latent_dim*2,))\n",
        "\n",
        "decoder_states_inputs = [decoder_state_input_h1, decoder_state_input_h2, decoder_state_input_h3]\n",
        "\n",
        "\n",
        "decoder_outputs, dh1 = decoder_1(decoder_inputs, initial_state=decoder_states_inputs[0])\n",
        "decoder_outputs, dh2 = decoder_2(decoder_outputs, initial_state=decoder_states_inputs[1])\n",
        "decoder_outputs, dh3 = decoder_3(decoder_outputs, initial_state=decoder_states_inputs[2])                            \n",
        "\n",
        "decoder_states = [dh1, dh2, dh3]\n",
        "\n",
        "\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVXAHnI4UF9I"
      },
      "source": [
        "#### Test Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpvi5m0ZVzZu"
      },
      "source": [
        "sample_idx = random.sample(range(len(input_texts)), 1000)\n",
        "test_texts = ['.sknahT', '.oellH', '?uoy era woH'] + [input_texts[i] for i in sample_idx]\n",
        "test_outputs = ['Merci.', 'Bonjour.', 'Comment vas-tu?'] + [target_texts[i] for i in sample_idx]\n",
        "\n",
        "def encode_texts_to_1hot_seq(input_texts):\n",
        "    input_seq = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
        "    for i, text in enumerate(input_texts):\n",
        "        for t, char in enumerate(text):\n",
        "            input_seq[i, t, input_token_index[char]] = 1.\n",
        "    return input_seq\n",
        "\n",
        "i=0\n",
        "BLEUscore=0\n",
        "\n",
        "for test_text, test_output in zip(test_texts, test_outputs):\n",
        "  \n",
        "    # Take one sequence (part of the training set) for trying out decoding.\n",
        "    input_seq = encode_texts_to_1hot_seq([test_text])\n",
        "    decoded_sentence = decode_sequence_gru(input_seq, encoder_model, decoder_model)\n",
        "    \n",
        "    # BLEU score\n",
        "    BLEUscore += nltk.translate.bleu_score.sentence_bleu([decoded_sentence.split(\" \")], test_output.split(\" \"), weights = [0.5, 0.5, 0, 0]) #(0.5, 0.5)\n",
        "    i+=1\n",
        "    \n",
        "#     print('---------------------------------------------------------')\n",
        "#     print('Input sentence: ', test_text[::-1])\n",
        "#     print('Decoded sentence: ', decoded_sentence)\n",
        "#     print('Actual translation: ', test_output)\n",
        "#     print('BLEU score is: ', BLEUscore)\n",
        "\n",
        "\n",
        "print(\"The BLEU score for this dataset is: \", BLEUscore*1.0/i)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}